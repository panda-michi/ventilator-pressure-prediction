{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"torch_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"nvSdFZ-ighgy"},"source":["!pip install pytorch-lightning\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"auSI5lSQhz1f"},"source":["import sys, os\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","import torch\n","from torch.utils.data import DataLoader\n","from torch import optim\n","from dataclasses import dataclass, asdict\n","from datetime import datetime\n","from time import time\n","from sklearn.model_selection import KFold\n","from tqdm.notebook import tqdm\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import AdamW\n","from transformers import get_cosine_schedule_with_warmup\n","import torch.cuda.amp as amp\n","\n","# workplace = '/content/drive/MyDrive/kaggle/codes/ventilator-pressure-prediction/'\n","# sys.path.append(workplace)\n","# from models.torch_lstm import simpleLSTM, TsLSTM, embedLSTM, dualDeepLSTM\n","# from utils import load_json, save_json\n","# from datasets import simpleData \n","# from functions import eval_metrics, torch_loss_metrics\n","# from utils import fixed_seed, worker_init_fn\n","# from functions.task import SequentialTrain\n","# from config import torch_model_conf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZrmOq6ZsQU-","executionInfo":{"status":"ok","timestamp":1637551819911,"user_tz":-540,"elapsed":306,"user":{"displayName":"panda mrt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi10Vhl_VatDLtm2ZuZxAREPYaBc_puPQ3Pkbhz7w=s64","userId":"00325656558560256374"}},"outputId":"b398ea36-9b42-45b5-bb45-16477defd7e2"},"source":["print(torch.__version__)\n","print(pl.__version__)\n","import transformers\n","print(transformers.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n","1.5.2\n","4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"DKbBIA3WlgKQ"},"source":["_supported_model = {\n","    'simple-lstm':  simpleLSTM,\n","    'embedded': embedLSTM,\n","    'transformer': TsLSTM,\n","    'dualdeep': dualDeepLSTM,\n","}\n","\n","_supported_criterion = {\n","    \"mse\": F.mse_loss,\n","    \"mae\": F.l1_loss,\n","}\n","\n","_supported_scheduler = {\n","    'reduce_plateau':   optim.lr_scheduler.ReduceLROnPlateau,\n","    'cosine':   optim.lr_scheduler.CosineAnnealingLR,\n","}\n","\n","_supported_optimizer = {\n","    'adam': optim.Adam,\n","    'sgd':  optim.SGD,\n","}\n","\n","_supported_dataset = {\n","    'simple': simpleData,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUXnWcoUnkjB"},"source":["@dataclass\n","class reduce_plateau_conf():\n","    factor: float = 0.5\n","    patience: int = 15\n","\n","@dataclass\n","class cosine_anneal_conf():\n","    eta_min: float = 1e-4\n","    T_max: int = 50\n","\n","@dataclass\n","class train_conf():\n","    seed: int = 221\n","\n","    model_name: str = 'embedded'\n","    model_param: torch_model_conf.embed_lstm() = torch_model_conf.embed_lstm(in_dim=23)\n","\n","    criterion: str = 'mae'\n","\n","    csv_dir: str = '/content/drive/MyDrive/kaggle/dataset/ventilator-pressure-prediction/train.csv'\n","\n","    # training parameters\n","    batch_size: int = 128\n","    num_workers: int = 2\n","    epoch: int = 500\n","\n","    # optimizer paramaters\n","    opt_name: str = 'adam'\n","    lr: float = 1e-3\n","    decay: int = -1\n","\n","    monitor: str = 'avg_val_loss'\n","\n","    early_stop_patience: int = 50\n","\n","    # scheduler parameters\n","    scheduler_name: str = 'reduce_plateau'\n","    scheduler_param: reduce_plateau_conf = reduce_plateau_conf()\n","    frequency: int = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaL3lj1Fi4SP"},"source":["def process_test():\n","    df = pd.read_csv('/content/drive/MyDrive/kaggle/datasets/ventilator-pressure-prediction/train.csv')\n","    print(len(df['breath_id'].unique()))\n","    x_col=['time_step','u_in','u_out','R','C',]\n","    y_col = ['pressure']\n","    idx = [i for i in range(100)]\n","    dataset = simpleData(df, idx, x_col, y_col)\n","    loader = DataLoader(dataset, 16)\n","    x, _, t = dataset[0]\n","    print(x.shape, t.shape)\n","\n","    model = _supported_model['embedded'](**asdict(torch_model_conf.embed_lstm(in_dim=5)))\n","    model.to(0)\n","\n","    criterion = F.l1_loss\n","    efunc = eval_metrics.mae\n","\n","    for batch in loader:\n","        x, _, t = batch\n","        print(x.shape, t.shape)\n","\n","        x = x.to(0)\n","        y, _ = model(x)\n","        print(y.shape)\n","\n","        loss = criterion(y.cpu(), t)\n","        print(loss)\n","\n","        y = y.cpu().detach().numpy()\n","        t = t.cpu().detach().numpy()\n","        print(efunc(y, t))\n","        break\n","\n","# process_test()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaBACixQpT05"},"source":["def setup_dataset(dname, csv_dir, data_cfg, tidx, vidx, x_col):\n","    df = pd.read_csv(csv_dir)\n","    y_col = ['pressure']\n","    train_dataset = _supported_dataset[dname](df, tidx, x_col, y_col, **asdict(data_cfg))\n","    valid_dataset = _supported_dataset[dname](df, vidx, x_col, y_col, **asdict(data_cfg))\n","\n","    return train_dataset, valid_dataset\n","\n","def setup_task(cfg, tidx, vidx):\n","    train_data, valid_data = setup_dataset(cfg.data_name, cfg.csv_dir, cfg.dataset_param, tidx, vidx)\n","\n","    train_loader = DataLoader(\n","            train_data,\n","            cfg.batch_size,\n","            shuffle = True,\n","            drop_last = True,\n","            num_workers = cfg.num_workers,\n","            pin_memory = True,\n","            worker_init_fn = worker_init_fn)\n","\n","    valid_loader = DataLoader(\n","            valid_data,\n","            cfg.batch_size,\n","            shuffle = False,\n","            num_workers = cfg.num_workers,\n","            pin_memory=True)\n","\n","    model = _supported_model[cfg.model_name](**asdict(cfg.model_param))\n","\n","    criterion = _supported_criterion[cfg.criterion]\n","    efunc = eval_metrics.mae\n","\n","    optimizer = _supported_optimizer[cfg.opt_name](model.parameters(), lr=cfg.lr, weight_decay=max(0, cfg.decay))\n","\n","    scheduler = {\n","        \"scheduler\": _supported_scheduler[cfg.scheduler_name](optimizer, **asdict(cfg.scheduler_param), verbose=True),\n","        \"monitor\": cfg.monitor,\n","        \"interval\": 'epoch',\n","        \"frequency\": cfg.frequency,\n","        }\n","\n","    return SequentialTrain(\n","        model = model,\n","        criterion = criterion,\n","        optimizers = optimizer,\n","        train_loader = train_loader,\n","        valid_loader = valid_loader,\n","        eval_func = efunc,\n","        schedulers = scheduler\n","    )\n","\n","def setup_trainer(cfg, out_dir):\n","    callbacks = []\n","    callbacks.append(\n","        pl.callbacks.ModelCheckpoint(\n","                monitor = cfg.monitor,\n","                save_weights_only=True,\n","                filename='best',\n","                auto_insert_metric_name=False,\n","                save_top_k = 1,\n","                save_last = True, \n","                verbose = True,\n","                mode = 'min',\n","                dirpath = f'{out_dir}ckpt/'))\n","    \n","    callbacks.append(\n","        pl.callbacks.EarlyStopping(\n","            monitor = cfg.monitor,\n","            patience = cfg.early_stop_patience,\n","            verbose = False,\n","            mode = 'min',\n","            min_delta = 0.0))\n","\n","    logger = TensorBoardLogger(f'{out_dir}logs/')\n","\n","    return pl.Trainer(gpus=[0], max_epochs=cfg.epoch, callbacks=callbacks, logger=logger, progress_bar_refresh_rate=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRBiQ4GXs7V7"},"source":["def apply_lightning():\n","    cfg = train_conf()\n","    now = datetime.now().strftime(\"%m%d%H%M%S\")\n","    out = f'{workplace}logs/{cfg.model_name}/{now}/'\n","    os.makedirs(out, exist_ok=True)\n","\n","    fixed_seed(cfg.seed)\n","\n","    k = 4 \n","    n = 75450\n","    indicies = np.arange(n)\n","    cv = KFold(n_splits=k, shuffle=True, random_state=cfg.seed)\n","\n","    for i, idx in enumerate(cv.split(indicies)):\n","        train_idx, valid_idx = idx\n","\n","        train_idx = indicies[train_idx]\n","        valid_idx = indicies[valid_idx]\n","\n","        out_i = f'{out}{i}/'\n","        os.makedirs(out_i, exist_ok=True)\n","        print(out_i)\n","        pickle.dump(train_idx, open(f'{out_i}train_idx.pkl', 'wb'))\n","        pickle.dump(valid_idx, open(f'{out_i}valid_idx.pkl', 'wb'))\n","        \n","        task = setup_task(cfg, train_idx, valid_idx)\n","        trainer = setup_trainer(cfg, out_i)\n","        trainer.fit(task)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rhw_VBexn2Z"},"source":["def train_step(model, loader, optimizer, scheduler, epoch):\n","    start_time = time()\n","    avg_loss = 0\n","    itr_cnt = 0\n","\n","    model.train()\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        itr_cnt += 1\n","        x, u_out, t = batch\n","        x, u_out, t = x.to(0), u_out.to(0), t.to(0)\n","        y_in, y_out = model(x)\n","        y = y_in*(1-u_out) + y_out*u_out\n","\n","        loss0 = F.l1_loss(y, t)\n","        loss1 = loss_metrics.mask_l1_loss(y_in, t, u_out < 0.5)\n","        loss2 = loss_metrics.mask_l1_loss(y_in, t, u_out > 0.5)\n","        loss = loss0 + loss1 + loss2\n","\n","        avg_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        # scheduler.step()\n","    \n","    print(f'epoch {epoch}: avg_train_loss {avg_loss / itr_cnt}, elapsed_time {time() - start_time}s')\n","\n","def valid_step(model, loader):\n","    model.eval()\n","    avg_loss = 0\n","    avg_mae = 0 \n","    avg_masked_mae = 0 \n","    itr_cnt = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in loader:\n","            itr_cnt += 1\n","            x, u_out, t = batch\n","            x, u_out, t = x.to(0), u_out.to(0), t.to(0)\n","\n","            y_in, y_out = model(x)\n","            y = y_in*(1-u_out) + y_out*u_out\n","\n","            loss0 = F.l1_loss(y, t)\n","            loss1 = loss_metrics.mask_l1_loss(y_in, t, u_out < 0.5)\n","            loss2 = loss_metrics.mask_l1_loss(y_in, t, u_out > 0.5)\n","            loss = loss0 + loss1 + loss2\n","\n","            avg_mae += F.l1_loss(y, t).item()\n","            avg_masked_mae += loss_metrics.mask_l1_loss(y, t, u_out < 0.5)\n","    \n","            avg_loss += loss.item()\n","    \n","    avg_loss /= itr_cnt\n","    avg_mae /= itr_cnt\n","    avg_masked_mae /= itr_cnt\n","\n","    print(f'\\t valid_loss {avg_loss}, mae {avg_mae}, masked_mae {avg_masked_mae}')\n","\n","    return avg_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnxIGKUZ_net"},"source":["def train_loop(cfg, tidx, vidx, x_col, out):\n","    train_data, valid_data = setup_dataset(cfg.data_name, cfg.csv_dir, cfg.dataset_param, tidx, vidx, x_col)\n","\n","    train_loader = DataLoader(\n","            train_data,\n","            cfg.batch_size,\n","            shuffle = True,\n","            drop_last = True,\n","            num_workers = cfg.num_workers,\n","            pin_memory = True,\n","            worker_init_fn = worker_init_fn)\n","\n","    valid_loader = DataLoader(\n","            valid_data,\n","            cfg.batch_size,\n","            shuffle = False,\n","            num_workers = cfg.num_workers,\n","            pin_memory=True)\n","    \n","    model = _supported_model[cfg.model_name](**asdict(cfg.model_param))\n","    model.to(0)\n","    optimizer = _supported_optimizer[cfg.opt_name](model.parameters(), lr=cfg.lr, weight_decay=max(0, cfg.decay))\n","\n","    # num_train_steps = int(len(train_loader) * cfg.epoch)\n","    # num_warmup_steps = int(num_train_steps / 10) \n","\n","    scheduler = _supported_scheduler[cfg.scheduler_name](optimizer, **asdict(cfg.scheduler_param), verbose=True)\n","\n","    best_model = None\n","    best_score = 10000000000\n","    not_update = 0\n","\n","    for ep in range(cfg.epoch):\n","        train_step(model, train_loader, optimizer, scheduler, ep)\n","        val_loss = valid_step(model, valid_loader)\n","        scheduler.step(val_loss)\n","\n","        if val_loss < best_score:\n","            print('update best model')\n","            not_update = 0\n","            best_score = val_loss\n","            best_model = model.state_dict()\n","            torch.save(model.state_dict(), f'{out}best_model')\n","        else :\n","            not_update += 1\n","        \n","        if not_update >= cfg.early_stop_patience:\n","            print('early stop triggered')\n","            break\n","    \n","    torch.cuda.empty_cache()\n","    return best_model\n","\n","def apply():\n","    cfg = train_conf()\n","    now = datetime.now().strftime('%m%d%H%M%S')\n","    out = f'{workplace}logs/{cfg.model_name}/{now}/'\n","    os.makedirs(out, exist_ok=True)\n","    save_json(asdict(cfg), out + 'params.json')\n","\n","    fixed_seed(cfg.seed)\n","\n","    x_cols=['time_step','u_in','u_out','R','C',]\n","    pickle.dump(x_cols, open(f'{out}x_cols.pkl', 'wb'))\n","\n","    k = 10\n","    n = 75450\n","    indicies = np.arange(n)\n","    cv = KFold(n_splits=k, shuffle=True, random_state=cfg.seed)\n","\n","    for i, idx in enumerate(cv.split(indicies)):\n","        print('###', i+1, '###')\n","        train_idx, valid_idx = idx\n","\n","        train_idx = indicies[train_idx]\n","        valid_idx = indicies[valid_idx]\n","\n","        out_i = f'{out}{i}/'\n","        os.makedirs(out_i, exist_ok=True)\n","        pickle.dump(train_idx, open(f'{out_i}train_idx.pkl', 'wb'))\n","        pickle.dump(valid_idx, open(f'{out_i}valid_idx.pkl', 'wb'))\n","\n","        best = train_loop(cfg, train_idx, valid_idx, x_cols, out_i)\n","        torch.save(best, f'{out_i}best_last')\n"],"execution_count":null,"outputs":[]}]}