{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"torch_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"nvSdFZ-ighgy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637551757825,"user_tz":-540,"elapsed":20387,"user":{"displayName":"panda mrt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi10Vhl_VatDLtm2ZuZxAREPYaBc_puPQ3Pkbhz7w=s64","userId":"00325656558560256374"}},"outputId":"a23e4468-f4df-4457-f6b9-ade870fa44cc"},"source":["!pip install pytorch-lightning\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.5.2-py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 5.1 MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 40.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 56.7 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n","\u001b[K     |████████████████████████████████| 329 kB 60.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n","Collecting PyYAML>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.2 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 49.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.42.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 60.9 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.7)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 69.2 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 70.7 MB/s \n","\u001b[?25hBuilding wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=e86cec78f91f0091a24ff0355606b0c138dc9bd2c899a19a96ca35ff3f927cc2\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built future\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.0 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.2 torchmetrics-0.6.0 yarl-1.7.2\n","Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 41.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 34.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.1.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"auSI5lSQhz1f","executionInfo":{"status":"ok","timestamp":1637551782335,"user_tz":-540,"elapsed":17096,"user":{"displayName":"panda mrt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi10Vhl_VatDLtm2ZuZxAREPYaBc_puPQ3Pkbhz7w=s64","userId":"00325656558560256374"}}},"source":["import sys, os\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","import torch\n","from torch.utils.data import DataLoader\n","from torch import optim\n","from dataclasses import dataclass, asdict\n","from datetime import datetime\n","from time import time\n","from sklearn.model_selection import KFold\n","from tqdm.notebook import tqdm\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import AdamW\n","from transformers import get_cosine_schedule_with_warmup\n","import torch.cuda.amp as amp\n","\n","# workplace = '/content/drive/MyDrive/kaggle/codes/ventilator-pressure-prediction/'\n","# sys.path.append(workplace)\n","# from models.torch_lstm import simpleLSTM, TsLSTM, embedLSTM, dualDeepLSTM\n","# from utils import load_json, save_json\n","# from datasets import simpleData \n","# from functions import eval_metrics, torch_loss_metrics\n","# from utils import fixed_seed, worker_init_fn\n","# from functions.task import SequentialTrain\n","# from config import torch_model_conf"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZrmOq6ZsQU-","executionInfo":{"status":"ok","timestamp":1637551819911,"user_tz":-540,"elapsed":306,"user":{"displayName":"panda mrt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi10Vhl_VatDLtm2ZuZxAREPYaBc_puPQ3Pkbhz7w=s64","userId":"00325656558560256374"}},"outputId":"b398ea36-9b42-45b5-bb45-16477defd7e2"},"source":["print(torch.__version__)\n","print(pl.__version__)\n","import transformers\n","print(transformers.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n","1.5.2\n","4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"DKbBIA3WlgKQ"},"source":["_supported_model = {\n","    'simple-lstm':  simpleLSTM,\n","    'embedded': embedLSTM,\n","    'transformer': TsLSTM,\n","    'dualdeep': dualDeepLSTM,\n","}\n","\n","_supported_criterion = {\n","    \"mse\": F.mse_loss,\n","    \"mae\": F.l1_loss,\n","}\n","\n","_supported_scheduler = {\n","    'reduce_plateau':   optim.lr_scheduler.ReduceLROnPlateau,\n","    'cosine':   optim.lr_scheduler.CosineAnnealingLR,\n","}\n","\n","_supported_optimizer = {\n","    'adam': optim.Adam,\n","    'sgd':  optim.SGD,\n","}\n","\n","_supported_dataset = {\n","    'simple': simpleData,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUXnWcoUnkjB"},"source":["@dataclass\n","class reduce_plateau_conf():\n","    factor: float = 0.5\n","    patience: int = 15\n","\n","@dataclass\n","class cosine_anneal_conf():\n","    eta_min: float = 1e-4\n","    T_max: int = 50\n","\n","@dataclass\n","class train_conf():\n","    seed: int = 221\n","\n","    model_name: str = 'embedded'\n","    model_param: torch_model_conf.embed_lstm() = torch_model_conf.embed_lstm(in_dim=23)\n","\n","    criterion: str = 'mae'\n","\n","    csv_dir: str = '/content/drive/MyDrive/kaggle/dataset/ventilator-pressure-prediction/train.csv'\n","\n","    # training parameters\n","    batch_size: int = 128\n","    num_workers: int = 2\n","    epoch: int = 500\n","\n","    # optimizer paramaters\n","    opt_name: str = 'adam'\n","    lr: float = 1e-3\n","    decay: int = -1\n","\n","    monitor: str = 'avg_val_loss'\n","\n","    early_stop_patience: int = 50\n","\n","    # scheduler parameters\n","    scheduler_name: str = 'reduce_plateau'\n","    scheduler_param: reduce_plateau_conf = reduce_plateau_conf()\n","    frequency: int = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaL3lj1Fi4SP"},"source":["def process_test():\n","    df = pd.read_csv('/content/drive/MyDrive/kaggle/datasets/ventilator-pressure-prediction/train.csv')\n","    print(len(df['breath_id'].unique()))\n","    x_col=['time_step','u_in','u_out','R','C',]\n","    y_col = ['pressure']\n","    idx = [i for i in range(100)]\n","    dataset = simpleData(df, idx, x_col, y_col)\n","    loader = DataLoader(dataset, 16)\n","    x, _, t = dataset[0]\n","    print(x.shape, t.shape)\n","\n","    model = _supported_model['embedded'](**asdict(torch_model_conf.embed_lstm(in_dim=5)))\n","    model.to(0)\n","\n","    criterion = F.l1_loss\n","    efunc = eval_metrics.mae\n","\n","    for batch in loader:\n","        x, _, t = batch\n","        print(x.shape, t.shape)\n","\n","        x = x.to(0)\n","        y, _ = model(x)\n","        print(y.shape)\n","\n","        loss = criterion(y.cpu(), t)\n","        print(loss)\n","\n","        y = y.cpu().detach().numpy()\n","        t = t.cpu().detach().numpy()\n","        print(efunc(y, t))\n","        break\n","\n","# process_test()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaBACixQpT05"},"source":["def setup_dataset(dname, csv_dir, data_cfg, tidx, vidx, x_col):\n","    df = pd.read_csv(csv_dir)\n","    y_col = ['pressure']\n","    train_dataset = _supported_dataset[dname](df, tidx, x_col, y_col, **asdict(data_cfg))\n","    valid_dataset = _supported_dataset[dname](df, vidx, x_col, y_col, **asdict(data_cfg))\n","\n","    return train_dataset, valid_dataset\n","\n","def setup_task(cfg, tidx, vidx):\n","    train_data, valid_data = setup_dataset(cfg.data_name, cfg.csv_dir, cfg.dataset_param, tidx, vidx)\n","\n","    train_loader = DataLoader(\n","            train_data,\n","            cfg.batch_size,\n","            shuffle = True,\n","            drop_last = True,\n","            num_workers = cfg.num_workers,\n","            pin_memory = True,\n","            worker_init_fn = worker_init_fn)\n","\n","    valid_loader = DataLoader(\n","            valid_data,\n","            cfg.batch_size,\n","            shuffle = False,\n","            num_workers = cfg.num_workers,\n","            pin_memory=True)\n","\n","    model = _supported_model[cfg.model_name](**asdict(cfg.model_param))\n","\n","    criterion = _supported_criterion[cfg.criterion]\n","    efunc = eval_metrics.mae\n","\n","    optimizer = _supported_optimizer[cfg.opt_name](model.parameters(), lr=cfg.lr, weight_decay=max(0, cfg.decay))\n","\n","    scheduler = {\n","        \"scheduler\": _supported_scheduler[cfg.scheduler_name](optimizer, **asdict(cfg.scheduler_param), verbose=True),\n","        \"monitor\": cfg.monitor,\n","        \"interval\": 'epoch',\n","        \"frequency\": cfg.frequency,\n","        }\n","\n","    return SequentialTrain(\n","        model = model,\n","        criterion = criterion,\n","        optimizers = optimizer,\n","        train_loader = train_loader,\n","        valid_loader = valid_loader,\n","        eval_func = efunc,\n","        schedulers = scheduler\n","    )\n","\n","def setup_trainer(cfg, out_dir):\n","    callbacks = []\n","    callbacks.append(\n","        pl.callbacks.ModelCheckpoint(\n","                monitor = cfg.monitor,\n","                save_weights_only=True,\n","                filename='best',\n","                auto_insert_metric_name=False,\n","                save_top_k = 1,\n","                save_last = True, \n","                verbose = True,\n","                mode = 'min',\n","                dirpath = f'{out_dir}ckpt/'))\n","    \n","    callbacks.append(\n","        pl.callbacks.EarlyStopping(\n","            monitor = cfg.monitor,\n","            patience = cfg.early_stop_patience,\n","            verbose = False,\n","            mode = 'min',\n","            min_delta = 0.0))\n","\n","    logger = TensorBoardLogger(f'{out_dir}logs/')\n","\n","    return pl.Trainer(gpus=[0], max_epochs=cfg.epoch, callbacks=callbacks, logger=logger, progress_bar_refresh_rate=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRBiQ4GXs7V7"},"source":["def apply_lightning():\n","    cfg = train_conf()\n","    now = datetime.now().strftime(\"%m%d%H%M%S\")\n","    out = f'{workplace}logs/{cfg.model_name}/{now}/'\n","    os.makedirs(out, exist_ok=True)\n","\n","    fixed_seed(cfg.seed)\n","\n","    k = 4 \n","    n = 75450\n","    indicies = np.arange(n)\n","    cv = KFold(n_splits=k, shuffle=True, random_state=cfg.seed)\n","\n","    for i, idx in enumerate(cv.split(indicies)):\n","        train_idx, valid_idx = idx\n","\n","        train_idx = indicies[train_idx]\n","        valid_idx = indicies[valid_idx]\n","\n","        out_i = f'{out}{i}/'\n","        os.makedirs(out_i, exist_ok=True)\n","        print(out_i)\n","        pickle.dump(train_idx, open(f'{out_i}train_idx.pkl', 'wb'))\n","        pickle.dump(valid_idx, open(f'{out_i}valid_idx.pkl', 'wb'))\n","        \n","        task = setup_task(cfg, train_idx, valid_idx)\n","        trainer = setup_trainer(cfg, out_i)\n","        trainer.fit(task)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rhw_VBexn2Z"},"source":["def train_step(model, loader, optimizer, scheduler, epoch):\n","    start_time = time()\n","    avg_loss = 0\n","    itr_cnt = 0\n","\n","    model.train()\n","    for batch in loader:\n","        optimizer.zero_grad()\n","        itr_cnt += 1\n","        x, u_out, t = batch\n","        x, u_out, t = x.to(0), u_out.to(0), t.to(0)\n","        y_in, y_out = model(x)\n","        y = y_in*(1-u_out) + y_out*u_out\n","\n","        loss0 = F.l1_loss(y, t)\n","        loss1 = loss_metrics.mask_l1_loss(y_in, t, u_out < 0.5)\n","        loss2 = loss_metrics.mask_l1_loss(y_in, t, u_out > 0.5)\n","        loss = loss0 + loss1 + loss2\n","\n","        avg_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        # scheduler.step()\n","    \n","    print(f'epoch {epoch}: avg_train_loss {avg_loss / itr_cnt}, elapsed_time {time() - start_time}s')\n","\n","def valid_step(model, loader):\n","    model.eval()\n","    avg_loss = 0\n","    avg_mae = 0 \n","    avg_masked_mae = 0 \n","    itr_cnt = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in loader:\n","            itr_cnt += 1\n","            x, u_out, t = batch\n","            x, u_out, t = x.to(0), u_out.to(0), t.to(0)\n","\n","            y_in, y_out = model(x)\n","            y = y_in*(1-u_out) + y_out*u_out\n","\n","            loss0 = F.l1_loss(y, t)\n","            loss1 = loss_metrics.mask_l1_loss(y_in, t, u_out < 0.5)\n","            loss2 = loss_metrics.mask_l1_loss(y_in, t, u_out > 0.5)\n","            loss = loss0 + loss1 + loss2\n","\n","            avg_mae += F.l1_loss(y, t).item()\n","            avg_masked_mae += loss_metrics.mask_l1_loss(y, t, u_out < 0.5)\n","    \n","            avg_loss += loss.item()\n","    \n","    avg_loss /= itr_cnt\n","    avg_mae /= itr_cnt\n","    avg_masked_mae /= itr_cnt\n","\n","    print(f'\\t valid_loss {avg_loss}, mae {avg_mae}, masked_mae {avg_masked_mae}')\n","\n","    return avg_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnxIGKUZ_net"},"source":["def train_loop(cfg, tidx, vidx, x_col, out):\n","    train_data, valid_data = setup_dataset(cfg.data_name, cfg.csv_dir, cfg.dataset_param, tidx, vidx, x_col)\n","\n","    train_loader = DataLoader(\n","            train_data,\n","            cfg.batch_size,\n","            shuffle = True,\n","            drop_last = True,\n","            num_workers = cfg.num_workers,\n","            pin_memory = True,\n","            worker_init_fn = worker_init_fn)\n","\n","    valid_loader = DataLoader(\n","            valid_data,\n","            cfg.batch_size,\n","            shuffle = False,\n","            num_workers = cfg.num_workers,\n","            pin_memory=True)\n","    \n","    model = _supported_model[cfg.model_name](**asdict(cfg.model_param))\n","    model.to(0)\n","    optimizer = _supported_optimizer[cfg.opt_name](model.parameters(), lr=cfg.lr, weight_decay=max(0, cfg.decay))\n","\n","    # num_train_steps = int(len(train_loader) * cfg.epoch)\n","    # num_warmup_steps = int(num_train_steps / 10) \n","\n","    scheduler = _supported_scheduler[cfg.scheduler_name](optimizer, **asdict(cfg.scheduler_param), verbose=True)\n","\n","    best_model = None\n","    best_score = 10000000000\n","    not_update = 0\n","\n","    for ep in range(cfg.epoch):\n","        train_step(model, train_loader, optimizer, scheduler, ep)\n","        val_loss = valid_step(model, valid_loader)\n","        scheduler.step(val_loss)\n","\n","        if val_loss < best_score:\n","            print('update best model')\n","            not_update = 0\n","            best_score = val_loss\n","            best_model = model.state_dict()\n","            torch.save(model.state_dict(), f'{out}best_model')\n","        else :\n","            not_update += 1\n","        \n","        if not_update >= cfg.early_stop_patience:\n","            print('early stop triggered')\n","            break\n","    \n","    torch.cuda.empty_cache()\n","    return best_model\n","\n","def apply():\n","    cfg = train_conf()\n","    now = datetime.now().strftime('%m%d%H%M%S')\n","    out = f'{workplace}logs/{cfg.model_name}/{now}/'\n","    os.makedirs(out, exist_ok=True)\n","    save_json(asdict(cfg), out + 'params.json')\n","\n","    fixed_seed(cfg.seed)\n","\n","    x_cols=['time_step','u_in','u_out','R','C',]\n","    pickle.dump(x_cols, open(f'{out}x_cols.pkl', 'wb'))\n","\n","    k = 10\n","    n = 75450\n","    indicies = np.arange(n)\n","    cv = KFold(n_splits=k, shuffle=True, random_state=cfg.seed)\n","\n","    for i, idx in enumerate(cv.split(indicies)):\n","        print('###', i+1, '###')\n","        train_idx, valid_idx = idx\n","\n","        train_idx = indicies[train_idx]\n","        valid_idx = indicies[valid_idx]\n","\n","        out_i = f'{out}{i}/'\n","        os.makedirs(out_i, exist_ok=True)\n","        pickle.dump(train_idx, open(f'{out_i}train_idx.pkl', 'wb'))\n","        pickle.dump(valid_idx, open(f'{out_i}valid_idx.pkl', 'wb'))\n","\n","        best = train_loop(cfg, train_idx, valid_idx, x_cols, out_i)\n","        torch.save(best, f'{out_i}best_last')\n"],"execution_count":null,"outputs":[]}]}